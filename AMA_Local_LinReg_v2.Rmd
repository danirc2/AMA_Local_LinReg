---
title: "Local Linear Regression: Estimating Conditional Variance"
author: "Azzarito Domenico, Daniel Reverter, Alexis Vendrix (Improved Solution)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Estimating the conditional variance by local linear regression

This document follows the procedure outlined in `statement.pdf` to estimate the conditional variance of aircraft weight based on its year of manufacture.

```{r libraries}
# Libraries
library(sm)         # For sm.regression
library(KernSmooth)   # For dpill (Direct Plug-In)
library(ggplot2)    # For plotting CV curves
source("locpolreg.R") # Assuming this file is in the same directory
```

## 1. Aircraft data and log transformation

We load the `aircraft` dataset and create the required log-transformed variables. We will use $x = \text{Yr}$ and $Y = \text{lgWeight}$ as our variables.

```{r data_prep}
# Load data
data(aircraft)

# Define variables for clarity, avoiding attach()
x <- aircraft$Yr
y <- log(aircraft$Weight)

# Define a smooth grid for plotting our final functions
eval.grid <- seq(min(x), max(x), length.out = 200)
```

## 2. Procedure overview

We follow the 4-step procedure from the documentation to estimate $\sigma^2(x)$ in the model $Y = m(x) + \sigma(x)\epsilon$.

1.  Fit $\hat{m}(x)$ to $(x_i, y_i)$.
2.  Calculate residuals $\hat{\epsilon}_i = y_i - \hat{m}(x_i)$ and transform them: $z_i = \log(\hat{\epsilon}_i^2)$.
3.  Fit $\hat{q}(x)$ to $(x_i, z_i)$.
4.  Estimate $\hat{\sigma}^2(x) = e^{\hat{q}(x)}$.

We will perform this procedure twice as requested.

---

## 3. Part 1: Using `loc.pol.reg` with LOOCV

First, we use the loc.pol.reg function. We will select the optimal bandwidth (the h parameter, which controls smoothness) using Leave-One-Out Cross-Validation (LOOCV). The bandwidth h that gives the best average prediction is chosen.

### CV Function
Here is the helper function for LOOCV.

```{r cv_function}
h.cv.gcv <- function(x,y,h.v = exp(seq(log(diff(range(x))/20),
                                       log(diff(range(x))/4),l=10)), 
                     p=1,type.kernel="normal"){
  n <- length(x)
  cv <- h.v*0
  gcv <- h.v*0
  for (i in (1:length(h.v))){
    h <- h.v[i]
    aux <- locpolreg(x=x,y=y,h=h,p=p,tg=x,
                     type.kernel=type.kernel, doing.plot=FALSE)
    S <- aux$S
    h.y <- aux$mtgr
    hii <- diag(S)
    av.hii <- mean(hii)
    cv[i] <- sum(((y-h.y)/(1-hii))^2)/n
    gcv[i] <- sum(((y-h.y)/(1-av.hii))^2)/n
  }
  return(list(h.v=h.v,cv=cv,gcv=gcv))
}
```

### Step 1.1: Fit mean function $\hat{m}(x)$

We select the optimal bandwidth $h_m$ for the mean function $m(x)$ using LOOCV. The plot shows the cross-validation error for different bandwidths; we pick the h that minimizes this error.

```{r part1_step1_cv, echo=FALSE, warning=FALSE}
cv.m <- h.cv.gcv(x, y, type.kernel="normal")
h.m <- cv.m$h.v[which.min(cv.m$cv)]
min.cv.m <- min(cv.m$cv)

# Plot CV curve
cv_data_m <- data.frame(h = cv.m$h.v, cv_error = cv.m$cv)
ggplot(cv_data_m, aes(x = h, y = cv_error)) +
  geom_line(color = "gray50") + geom_point(color = "gray50") +
  geom_vline(xintercept = h.m, linetype = "dashed", color = "red", linewidth = 1) +
  geom_point(aes(x = h.m, y = min.cv.m), color = "red", size = 3) +
  scale_x_log10() +
  labs(title = "LOOCV for Mean Function m(x)",
       x = "Bandwidth (h) on log scale", y = "Cross-Validation Error") +
  annotate("text", x = h.m, y = min.cv.m + 0.05 * min.cv.m,
           label = paste("Optimal h_m =", round(h.m, 4)), color = "red", hjust = -0.1) +
  theme_bw(base_family = "serif") + theme(plot.title = element_text(hjust = 0.5))
```
The optimal bandwidth for the mean function is $h_m = `r round(h.m, 4)`$.

Now, we fit $\hat{m}(x)$ on our smooth `eval.grid` for plotting.

```{r part1_step1_fit}
m.hat.fit <- locpolreg(x, y, h = h.m, tg = eval.grid)
m.hat.grid <- m.hat.fit$mtgr
```

### Step 1.2: Calculate residuals $z_i$

To get residuals, we must fit $\hat{m}(x_i)$ at the **original data points** $x_i$, not the grid. This gives us one residual $\hat{\epsilon}_i$ for each data point $(x_i, y_i)$.

```{r part1_step2, fig.show = 'hide'}
m.hat.points <- locpolreg(x, y, h = h.m, tg = x)$mtgr
eps.hat <- y - m.hat.points
z <- log(eps.hat^2)
```

### Step 1.3: Fit log-variance function $\hat{q}(x)$
Now we repeat the bandwidth selection, but this time for the new regression of $z_i = \log(\hat{\epsilon}_i^2)$ on $x_i$. This will give us a new optimal bandwidth, $h_q$.

We fit a nonparametric regression to $(x_i, z_i)$ to get $\hat{q}(x)$. We find a new optimal bandwidth $h_q$ using LOOCV.

```{r part1_step3, echo=FALSE, warning=FALSE}
# Run CV for the variance function q(x)
cv.q <- h.cv.gcv(x, z, type.kernel="normal")
h.q <- cv.q$h.v[which.min(cv.q$cv)]
min.cv.q <- min(cv.q$cv)

# Plot CV curve
cv_data_q <- data.frame(h = cv.q$h.v, cv_error = cv.q$cv)
ggplot(cv_data_q, aes(x = h, y = cv_error)) +
  geom_line(color = "gray50") + geom_point(color = "gray50") +
  geom_vline(xintercept = h.q, linetype = "dashed", color = "red", linewidth = 1) +
  geom_point(aes(x = h.q, y = min.cv.q), color = "red", size = 3) +
  scale_x_log10() +
  labs(title = "LOOCV for Log-Variance Function q(x)",
       x = "Bandwidth (h) on log scale", y = "Cross-Validation Error") +
  annotate("text", x = h.q, y = min.cv.q + 0.05 * min.cv.q,
           label = paste("Optimal h_q =", round(h.q, 4)), color = "red", hjust = -0.1) +
  theme_bw(base_family = "serif") + theme(plot.title = element_text(hjust = 0.5))
```
The optimal bandwidth for the log-variance function is $h_q = `r round(h.q, 4)`$.

Now, we fit $\hat{q}(x)$ on our smooth `eval.grid` for plotting.

```{r part1_step3_fit}
# Fit q(x) on the smooth grid for plotting
q.hat.fit <- locpolreg(x, z, h = h.q, tg = eval.grid)
q.hat.grid <- q.hat.fit$mtgr
```

### Step 1.4: Estimate $\hat{\sigma}^2(x)$ and plot
We complete the procedure by exponentiating $\hat{q}(x)$ to get our final variance estimate $\hat{\sigma}^2(x)$ and our standard deviation estimate $\hat{\sigma}(x)$.


```{r part1_step4}
# Get variance and standard deviation estimates on the grid
sigma2.hat.grid <- exp(q.hat.grid)
sigma.hat.grid  <- sqrt(sigma2.hat.grid)
```

**Plot 1: Squared residuals vs. Year**

```{r part1_plot1, fig.height=5, fig.width=7, echo=FALSE}
par(family = "serif")
plot(x, eps.hat^2, col="grey", pch=20,
     xlab="Year (x = Yr)", ylab=expression(hat(epsilon)^2),
     main=expression(paste("Squared Residuals vs. Year with ", hat(sigma)^2 (x), " (loc.pol.reg)")))
lines(eval.grid, sigma2.hat.grid, col="blue", lwd=2)
legend("topright", 
       legend = c(expression(hat(epsilon)[i]^2), expression(hat(sigma)^2 (x))),
       col = c("grey","blue"), lty = c(NA,1), pch = c(20,NA), bty = "n")
```
This plot shows the "noisy" variance proxies (the squared residuals $\hat{\epsilon}_i^2$ in grey) against the year. The solid blue line is our smooth estimate of the underlying conditional variance function, $\hat{\sigma}^2(x)$. This line captures the trend in the variance.

**Plot 2: Mean function $\hat{m}(x)$ with $95\%$ bands** 

```{r part1_plot2, fig.height=5, fig.width=7, echo=FALSE, echo=FALSE}
# Calculate bands
upper.band <- m.hat.grid + 1.96 * sigma.hat.grid
lower.band <- m.hat.grid - 1.96 * sigma.hat.grid

# Calculate dynamic y-axis limits to ensure bands fit
plot.ylim <- range(y, upper.band, lower.band)

par(family = "serif")
plot(eval.grid, m.hat.grid, type="l", lwd=2, col="black",
     main=expression(paste(hat(m)(x), " \U00B1 1.96", hat(sigma)(x), " (loc.pol.reg)")),
     xlab="Year (x = Yr)", ylab="log(Weight) (y = lgWeight)",
     ylim = plot.ylim)
lines(eval.grid, upper.band, col="red", lty=2, lwd=1.5)
lines(eval.grid, lower.band, col="red", lty=2, lwd=1.5)
points(x, y, col="grey", pch=20) # Add raw data
legend("topright",
       legend = c(expression(hat(m)(x)), expression(paste("95% Bands: ", hat(m)(x) %+-% 1.96*hat(sigma)(x)))),
       lty = c(1,2), col = c("black","red"), lwd = c(2,1.5), bty = "n")
```

---

## 4. Part 2: Using `sm.regression` with DPI

Second, we repeat the entire procedure using the `sm.regression` function from the `sm` library. The key difference here is the bandwidth selection. Instead of the slow LOOCV, we use `dpill` from the `KernSmooth` library. Direct Plug-In (DPI) is an automatic method that estimates the optimal bandwidth based on the data's properties (like its curvature) rather than iterating through many possibilities.

### Step 2.1: Fit mean function $\hat{m}(x)$

We select $h_m$ using `dpill` and fit $\hat{m}(x)$ on the smooth `eval.grid`.

```{r part2_step1, results='asis'}
h.m.sm <- dpill(x, y)
cat(sprintf("DPI bandwidth for m(x) is h_m = %.4f\n", h.m.sm))

# Fit m(x) on the smooth grid for plotting
sm.m.fit <- sm.regression(x, y, h = h.m.sm, eval.points = eval.grid, model = "none")
m.hat.sm.grid <- sm.m.fit$estimate
```

### Step 2.2: Calculate residuals $z_i$

To get residuals, we must evaluate the fit at the **original data points** $x_i$.

```{r part2_step2}
# Fit m(x) at the original data points (eval.points=x) to get residuals
sm.m.points <- sm.regression(x, y, h = h.m.sm, eval.points = x, model = "none")
eps.hat.sm <- y - sm.m.points$estimate
z.sm <- log(eps.hat.sm^2) # Transformed residuals
```

### Step 2.3: Fit log-variance function $\hat{q}(x)$

We select $h_q$ using `dpill` for the $(x_i, z_i)$ data and fit $\hat{q}(x)$ on the smooth `eval.grid`.

```{r part2_step3, results='asis'}
h.q.sm <- dpill(x, z.sm)
cat(sprintf("DPI bandwidth for q(x) is h_q = %.4f\n", h.q.sm))

# Fit q(x) on the smooth grid for plotting
sm.q.fit <- sm.regression(x, z.sm, h = h.q.sm, eval.points = eval.grid, model = "none")
q.hat.sm.grid <- sm.q.fit$estimate
```

### Step 2.4: Estimate $\hat{\sigma}^2(x)$ and Plot

We estimate $\hat{\sigma}^2(x) = e^{\hat{q}(x)}$ and create the required plots.

```{r part2_step4}
# Get variance and standard deviation estimates on the grid
sigma2.hat.sm.grid <- exp(q.hat.sm.grid)
sigma.hat.sm.grid  <- sqrt(sigma2.hat.sm.grid)
```

**Plot 1: Squared Residuals vs. Year**
This plot is very similar to the one from Part 1. 
The grey dots are identical, but the blue line ($\hat{\sigma}^2(x)$) is slightly different due to the different function and bandwidth selector.

```{r part2_plot1, fig.height=5, fig.width=7, echo=FALSE}
par(family = "serif")
plot(x, eps.hat.sm^2, col="grey", pch=20,
     xlab="Year (x = Yr)", ylab=expression(hat(epsilon)^2),
     main=expression(paste("Squared Residuals vs. Year with ", hat(sigma)^2(x), " (sm.regression)")))
# Plot the smooth line using the grid estimates
lines(eval.grid, sigma2.hat.sm.grid, col="blue", lwd=2)
legend("topright", 
       legend = c(expression(hat(epsilon)[i]^2), expression(hat(sigma)^2(x))),
       col = c("grey","blue"), lty = c(NA,1), pch = c(20,NA), bty = "n")
```

**Plot 2: Mean Function $\hat{m}(x)$ with $95\%$ Bands**

```{r part2_plot2, fig.height=5, fig.width=7}
# Calculate bands
upper.band.sm <- m.hat.sm.grid + 1.96 * sigma.hat.sm.grid
lower.band.sm <- m.hat.sm.grid - 1.96 * sigma.hat.sm.grid

# Calculate dynamic y-axis limits
plot.ylim.sm <- range(y, upper.band.sm, lower.band.sm)

par(family = "serif")
# Plot the smooth line using the grid estimates
plot(eval.grid, m.hat.sm.grid, type="l", lwd=2, col="black",
     main=expression(paste(hat(m)(x), " \U00B1 1.96", hat(sigma)(x), " (sm.regression)")),
     xlab="Year (x = Yr)", ylab="log(Weight) (y = lgWeight)",
     ylim = plot.ylim.sm)
lines(eval.grid, upper.band.sm, col="red", lty=2, lwd=1.5)
lines(eval.grid, lower.band.sm, col="red", lty=2, lwd=1.5)
points(x, y, col="grey", pch=20) # Add raw data
legend("topright",
       legend = c(expression(hat(m)(x)), expression(paste("95% Bands: ", 
                                          hat(m)(x) %+-% 1.96*hat(sigma)(x)))),
       lty = c(1,2), col = c("black","red"), lwd = c(2,1.5), bty = "n")
```

---

## 5. Comparison of methods: `loc.pol.reg` vs. `sm.regression`

Both procedures were successful in modeling the heteroscedasticity of the data. The final plots in Part 1 and Part 2 are conceptually identical: they both show that the average `lgWeight` increases with `Yr`, and more importantly, that the *variance* also increases, which is visualized by the fanning-out prediction bands.

However, the two methods differ significantly in their approach and computational cost, primarily due to **bandwidth selection**.

### Bandwidth selection (LOOCV vs. DPI)

This is the most important distinction between the two parts.

* **Part 1: `loc.pol.reg` with LOOCV**
    * We used Leave-One-Out Cross-Validation (LOOCV), a "brute-force" but intuitive method.
    * **Pro:** It directly tests which bandwidth `h` gives the best out-of-sample prediction error for the specific data we have.
    * **Con:** It could be slow as it requires re-fitting the model many times for each potential bandwidth.

* **Part 2: `sm.regression` with DPI**
    * We used Direct Plug-In (DPI) via the `dpill` function.
    * **Pro:** It is an automatic, formula-based method that estimates the optimal bandwidth from properties of the data (like its estimated curvature). It is **extremely fast**.
    * **Con:** It relies on asymptotic formulas and assumptions, which may not always be a perfect fit for smaller or unusual datasets.

### Comparison of selected sandwidths

Because the selection methods are different, they will almost always result in different optimal bandwidths.

```{r comparison_table, echo=FALSE}
# Create a data frame for comparison
bw_data <- data.frame(
  Function = c("Mean m(x)", "Log-Variance q(x)"),
  LOOCV_h = c(h.m, h.q),
  DPI_h = c(h.m.sm, h.q.sm)
)

# Use knitr::kable for a clean table in the PDF
knitr::kable(bw_data, 
             col.names = c("Function", "h (LOOCV)", "h (DPI)"),
             caption = "Comparison of Optimal Bandwidths",
             digits = 4)
```